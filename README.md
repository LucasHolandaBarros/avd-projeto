# AVD - Projeto de Machine Learning para PrevisÃ£o de Umidade Relativa

**Disciplina**: AnÃ¡lise e VisualizaÃ§Ã£o de Dados - 2025.2  
**InstituiÃ§Ã£o**: CESAR School  
**Problema Escolhido**: 7.6 - Prever Umidade Relativa

## ğŸ‘¥ Equipe

<!-- ADICIONE AQUI OS NOMES E USUÃRIOS DO GITHUB DOS MEMBROS DO GRUPO -->
<!-- Exemplo: -->
<!-- - Nome Completo (@usuario_github) -->
<!-- - Nome Completo (@usuario_github) -->

---

Projeto de Machine Learning para previsÃ£o de umidade relativa do ar utilizando dados meteorolÃ³gicos do INMET (Instituto Nacional de Meteorologia). O projeto utiliza Random Forest para modelagem e MLflow para tracking de experimentos, com infraestrutura containerizada via Docker Compose.

## ğŸ“‹ Ãndice

- [Equipe](#-equipe)
- [DescriÃ§Ã£o](#-descriÃ§Ã£o)
- [Arquitetura](#-arquitetura)
- [PrÃ©-requisitos](#-prÃ©-requisitos)
- [ConfiguraÃ§Ã£o Inicial](#-configuraÃ§Ã£o-inicial)
- [ExecuÃ§Ã£o do Projeto](#-execuÃ§Ã£o-do-projeto)
- [Como Usar](#-como-usar)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [Troubleshooting](#-troubleshooting)

## ğŸ¯ DescriÃ§Ã£o

Este projeto processa dados meteorolÃ³gicos de estaÃ§Ãµes do Nordeste do Brasil (INMET) e treina modelos de Machine Learning para prever a umidade relativa do ar. O sistema inclui:

- **Processamento de dados**: Limpeza e preparaÃ§Ã£o de dados meteorolÃ³gicos
- **Treinamento de modelos**: Random Forest Regressor com pipeline de prÃ©-processamento
- **Tracking de experimentos**: MLflow para versionamento e comparaÃ§Ã£o de modelos
- **Armazenamento**: MinIO (S3-compatible) para artefatos e MySQL para metadados
- **API**: FastAPI para upload de dados para S3
- **IntegraÃ§Ã£o**: Snowflake para armazenamento de dados processados

## ğŸ—ï¸ Arquitetura

O projeto utiliza Docker Compose para orquestrar os seguintes serviÃ§os:

- **FastAPI**: Interface de ingestÃ£o dos dados do INMET e integraÃ§Ã£o com S3
- **MinIO**: Armazenamento de dados brutos e modelos (S3-compatible)
- **Snowflake**: EstruturaÃ§Ã£o de dados tratados (configuraÃ§Ã£o via variÃ¡veis de ambiente)
- **Jupyter**: Ambiente de anÃ¡lise, limpeza e modelagem preditiva
- **MLflow**: Registro e versionamento dos modelos de ML
- **MySQL**: Banco de dados para metadados do MLflow

### Fluxo do Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI   â”‚â”€â”€â”€â”€â–¶â”‚  MinIO/S3   â”‚â”€â”€â”€â”€â–¶â”‚  Snowflake  â”‚â”€â”€â”€â”€â–¶â”‚   Jupyter   â”‚
â”‚  (IngestÃ£o) â”‚     â”‚ (Armazen.)  â”‚     â”‚ (Estrutura)  â”‚     â”‚ (Modelagem) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                                                      â”‚
                                                                      â–¼
                                                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                               â”‚   MLflow    â”‚
                                                               â”‚  (Tracking) â”‚
                                                               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                                                      â”‚
                                                                      â–¼
                                                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                               â”‚    MinIO    â”‚
                                                               â”‚  (Artifacts) â”‚
                                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Fluxo completo:**

1. FastAPI recebe dados do INMET (API/CSV) e armazena em S3/MinIO
2. Dados sÃ£o estruturados em Snowflake
3. Jupyter Notebook lÃª da base estruturada, trata e treina um modelo
4. Modelo Ã© versionado no MLflow e exportado novamente para S3/MinIO
5. Dashboard (ThingsBoard/Trendz) consome os dados e mostra visualizaÃ§Ãµes

## ğŸ“¦ PrÃ©-requisitos

Antes de comeÃ§ar, certifique-se de ter instalado:

- **Docker** (versÃ£o 20.10 ou superior)
- **Docker Compose** (versÃ£o 2.0 ou superior)
- **Git** (para clonar o repositÃ³rio)

### Verificar instalaÃ§Ã£o

```bash
docker --version
docker-compose --version
git --version
```

## âš™ï¸ ConfiguraÃ§Ã£o Inicial

### 1. Clonar o repositÃ³rio

```bash
git clone <url-do-repositorio>
cd avd-projeto
```

### 2. Configurar variÃ¡veis de ambiente

Copie o arquivo de exemplo e configure as variÃ¡veis:

```bash
cp .env-example .env
```

Edite o arquivo `.env` e preencha as seguintes variÃ¡veis:

#### VariÃ¡veis obrigatÃ³rias (para funcionamento local):

```env
# MinIO (armazenamento local)
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin
MINIO_ACCESS_KEY_ID=minioadmin
MINIO_SECRET_ACCESS_KEY=minioadmin

# MySQL
MYSQL_ROOT_PASSWORD=root
MYSQL_DATABASE=mlflow
MYSQL_USER=mlflow_user
MYSQL_PASSWORD=mlflow_pass

# MLflow
MLFLOW_MINIO_ENDPOINT_URL=http://minio:9000
MLFLOW_ARTIFACT_BUCKET=mlflow
MLFLOW_BACKEND_URI=mysql+pymysql://mlflow_user:mlflow_pass@mysql:3306/mlflow
```

#### VariÃ¡veis opcionais (para integraÃ§Ã£o com AWS/Snowflake):

```env
# AWS (para upload para S3)
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_DEFAULT_REGION=
S3_BUCKET=
S3_PATH=

# Snowflake (para fetch de dados)
SNOWFLAKE_USER=
SNOWFLAKE_PASSWORD=
SNOWFLAKE_ACCOUNT=
SNOWFLAKE_WAREHOUSE=
SNOWFLAKE_DATABASE=
SNOWFLAKE_SCHEMA=
SNOWFLAKE_TABLE=
```

> **Nota**: As variÃ¡veis do MinIO e MySQL acima sÃ£o suficientes para executar o projeto localmente. As variÃ¡veis da AWS e Snowflake sÃ£o necessÃ¡rias apenas se vocÃª quiser usar essas integraÃ§Ãµes.

## ğŸš€ ExecuÃ§Ã£o do Projeto

### Passo 1: Iniciar os containers

No diretÃ³rio raiz do projeto, execute:

```bash
docker-compose up -d
```

Este comando irÃ¡:

- Construir as imagens Docker necessÃ¡rias
- Iniciar todos os serviÃ§os (Jupyter, MLflow, MinIO, MySQL)
- Configurar os buckets do MinIO automaticamente

### Passo 2: Verificar se os serviÃ§os estÃ£o rodando

```bash
docker-compose ps
```

Todos os containers devem estar com status `Up`. VocÃª deve ver:

- `jupyter_app` (porta 8888)
- `mlflow` (porta 5000)
- `minio` (portas 9000 e 9001)
- `mysql` (porta 13308)
- `fastapi_app` (porta 8000)

### Passo 3: Acessar os serviÃ§os

ApÃ³s alguns segundos, os serviÃ§os estarÃ£o disponÃ­veis:

- **Jupyter Notebook**: http://localhost:8888
- **MLflow UI**: http://localhost:5000
- **MinIO Console**: http://localhost:9001
  - UsuÃ¡rio: `minioadmin`
  - Senha: `minioadmin`
- **FastAPI**: http://localhost:8000
- **MySQL**: `localhost:13308`

### Passo 4: Executar o notebook de treinamento

1. Acesse o Jupyter em http://localhost:8888
2. Navegue atÃ© `model_training.ipynb`
3. Execute todas as cÃ©lulas (Menu: `Cell` â†’ `Run All` ou `Shift+Enter` em cada cÃ©lula)

O notebook irÃ¡:

- Carregar os dados processados
- Treinar um modelo Random Forest
- Registrar o experimento no MLflow
- Salvar artefatos (modelo e prediÃ§Ãµes) no MinIO

## ğŸ“– Como Usar

### Treinar um modelo

1. **Preparar os dados**: Certifique-se de que o arquivo CSV processado estÃ¡ em `/data/processed/`
2. **Abrir o notebook**: Acesse `model_training.ipynb` no Jupyter
3. **Ajustar parÃ¢metros** (opcional):
   - `FILE_PATH`: Caminho do arquivo CSV
   - `FEATURES`: Features utilizadas no modelo
   - `TARGET`: VariÃ¡vel alvo
4. **Executar**: Execute todas as cÃ©lulas do notebook

### Visualizar experimentos no MLflow

1. Acesse http://localhost:5000
2. Navegue atÃ© o experimento "Default"
3. Clique em um run para ver:
   - ParÃ¢metros do modelo
   - MÃ©tricas (MAE, etc.)
   - Artefatos (modelo treinado, CSV de prediÃ§Ãµes)

### Upload de dados para S3 (opcional)

Se vocÃª configurou as credenciais AWS:

```bash
curl -X POST "http://localhost:8000/upload"
```

Ou use o script de pipeline:

```bash
bash scripts/pipeline.sh --upload
```

### Processar dados do Snowflake (opcional)

Se vocÃª configurou as credenciais do Snowflake:

```bash
docker-compose exec jupyter python /scripts/fetch_from_snowflake.py
```

## ğŸ“ Estrutura do Projeto

```
avd-projeto/
â”œâ”€â”€ data/                          # Dados do projeto
â”‚   â”œâ”€â”€ raw/                       # Dados brutos
â”‚   â”œâ”€â”€ processed/                 # Dados processados
â”‚   â””â”€â”€ artifacts/                 # Modelos e prediÃ§Ãµes gerados
â”œâ”€â”€ jupyter_app/                   # Container Jupyter (jupyterlab/)
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ notebooks/                 # Notebooks de tratamento e modelagem
â”‚       â”œâ”€â”€ model_training.ipynb   # Notebook principal de treinamento
â”‚       â”œâ”€â”€ eda.ipynb              # AnÃ¡lise exploratÃ³ria
â”‚       â””â”€â”€ mae_analysis.ipynb     # AnÃ¡lise de mÃ©tricas
â”œâ”€â”€ mlflow_app/                    # Container MLflow (mlflow/)
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ fastapi_app/                   # Container FastAPI (fastapi/)
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ scripts/                        # Scripts auxiliares
â”‚   â”œâ”€â”€ pipeline.sh                # Script de pipeline completo
â”‚   â”œâ”€â”€ fetch_from_snowflake.py    # Fetch de dados do Snowflake
â”‚   â”œâ”€â”€ process_and_merge_datasets.py  # Processamento de datasets
â”‚   â””â”€â”€ snowflake_script_adaptado.sql  # Script SQL do Snowflake
â”œâ”€â”€ sql_scripts/                    # Scripts SQL de estruturaÃ§Ã£o e consultas
â”‚   â””â”€â”€ (a criar)
â”œâ”€â”€ trendz/                         # Dashboards exportados do Trendz
â”‚   â””â”€â”€ (a criar)
â”œâ”€â”€ reports/                        # RelatÃ³rios e resultados
â”‚   â””â”€â”€ (relatÃ³rio tÃ©cnico em PDF a criar)
â”œâ”€â”€ docker-compose.yml             # OrquestraÃ§Ã£o dos contÃªineres
â”œâ”€â”€ .env-example                   # Exemplo de variÃ¡veis de ambiente
â”œâ”€â”€ README.md                      # Este arquivo
â””â”€â”€ LICENSE                        # LicenÃ§a do projeto
```

## ğŸ”§ Troubleshooting

### Erro: "ModuleNotFoundError: No module named 'sklearn'"

**SoluÃ§Ã£o**: Reconstrua o container do Jupyter:

```bash
docker-compose build jupyter
docker-compose up -d jupyter
```

### Erro: "Access Denied" ao salvar artefatos no MLflow

**SoluÃ§Ã£o**: Verifique se as variÃ¡veis de ambiente estÃ£o configuradas corretamente:

```bash
docker-compose exec jupyter env | grep -E "AWS_|MLFLOW_"
```

Certifique-se de que:

- `AWS_ACCESS_KEY_ID` e `AWS_SECRET_ACCESS_KEY` estÃ£o configurados
- `MLFLOW_S3_ENDPOINT_URL` estÃ¡ sem barra no final

### Erro: "Invalid Host header" no MLflow

**SoluÃ§Ã£o**: O MLflow jÃ¡ estÃ¡ configurado com `--allowed-hosts`. Se o erro persistir, verifique os logs:

```bash
docker-compose logs mlflow
```

### Container nÃ£o inicia

**SoluÃ§Ã£o**: Verifique os logs do container:

```bash
docker-compose logs <nome-do-container>
```

Exemplos:

```bash
docker-compose logs jupyter
docker-compose logs mlflow
docker-compose logs mysql
```

### Porta jÃ¡ em uso

**SoluÃ§Ã£o**: Pare outros serviÃ§os que possam estar usando as portas ou altere as portas no `docker-compose.yml`.

### Limpar tudo e comeÃ§ar do zero

```bash
# Parar e remover containers
docker-compose down

# Remover volumes (CUIDADO: apaga dados)
docker-compose down -v

# Reconstruir tudo
docker-compose build --no-cache
docker-compose up -d
```

## ğŸ“Š Funcionamento BÃ¡sico

### Fluxo de Dados

1. **Dados brutos** â†’ `data/raw/` ou `data/2023/`, `data/2024/`
2. **Processamento** â†’ Scripts em `scripts/` processam e limpam os dados
3. **Dados processados** â†’ `data/processed/concatenado_clean_*.csv`
4. **Treinamento** â†’ Notebook `model_training.ipynb` treina o modelo
5. **Registro** â†’ MLflow registra parÃ¢metros, mÃ©tricas e artefatos
6. **Armazenamento** â†’ Artefatos salvos no MinIO e metadados no MySQL

### Modelo de Machine Learning

- **Algoritmo**: Random Forest Regressor
- **Features**: Temperatura do bulbo seco, PressÃ£o atmosfÃ©rica, RadiaÃ§Ã£o global
- **Target**: Umidade relativa do ar
- **MÃ©trica**: MAE (Mean Absolute Error)
- **PrÃ©-processamento**: StandardScaler

### Artefatos Gerados

ApÃ³s executar o notebook, vocÃª terÃ¡:

- `rf_model.pkl`: Modelo treinado
- `predictions_partial.csv`: PrediÃ§Ãµes do conjunto de teste
- `predictions_full.csv`: PrediÃ§Ãµes do conjunto completo
- Registro completo no MLflow com histÃ³rico de experimentos

## ğŸ“ Notas Importantes

- O projeto usa **MinIO local** para armazenamento de artefatos (nÃ£o requer AWS)
- O **MySQL** armazena apenas metadados do MLflow (experimentos, runs, etc.)
- Os **dados processados** devem estar em `/data/processed/` dentro do container
- O **Jupyter** nÃ£o requer autenticaÃ§Ã£o (token vazio) para facilitar o desenvolvimento
- Todos os serviÃ§os estÃ£o na mesma rede Docker (`app-net`)

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

---

**Desenvolvido para o projeto AVD - CESAR School - 2025.2**
